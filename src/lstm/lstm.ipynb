{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM) Network for Text Classification\n",
    "\n",
    "This notebook implements LSTM models for text classification using the NusaX-Sentiment dataset (Bahasa Indonesia). We'll experiment with different hyperparameters and compare with a from-scratch implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load NusaX-Sentiment dataset\n",
    "def load_nusax_sentiment_data():\n",
    "    \"\"\"\n",
    "    Load NusaX-Sentiment dataset for Indonesian language\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to load from Hugging Face datasets\n",
    "        from datasets import load_dataset\n",
    "        \n",
    "        # Load Indonesian sentiment dataset\n",
    "        dataset = load_dataset(\"indonlp/nusax_senti\", \"indonesian\")\n",
    "        \n",
    "        # Extract train, validation, and test sets\n",
    "        train_data = dataset['train']\n",
    "        val_data = dataset['validation'] \n",
    "        test_data = dataset['test']\n",
    "        \n",
    "        # Convert to pandas DataFrames\n",
    "        train_df = pd.DataFrame({\n",
    "            'text': train_data['text'],\n",
    "            'label': train_data['label']\n",
    "        })\n",
    "        \n",
    "        val_df = pd.DataFrame({\n",
    "            'text': val_data['text'],\n",
    "            'label': val_data['label']\n",
    "        })\n",
    "        \n",
    "        test_df = pd.DataFrame({\n",
    "            'text': test_data['text'],\n",
    "            'label': test_data['label']\n",
    "        })\n",
    "        \n",
    "        return train_df, val_df, test_df\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"datasets library not found. Installing...\")\n",
    "        !pip install datasets\n",
    "        from datasets import load_dataset\n",
    "        return load_nusax_sentiment_data()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        print(\"Creating synthetic dataset for demonstration...\")\n",
    "        return create_synthetic_sentiment_data()\n",
    "\n",
    "def create_synthetic_sentiment_data():\n",
    "    \"\"\"\n",
    "    Create synthetic sentiment data if real dataset is not available\n",
    "    \"\"\"\n",
    "    # Synthetic Indonesian text samples\n",
    "    positive_texts = [\n",
    "        \"Saya sangat senang dengan produk ini\",\n",
    "        \"Film ini benar-benar luar biasa dan menghibur\",\n",
    "        \"Pelayanan yang sangat memuaskan dan ramah\",\n",
    "        \"Makanan di restoran ini enak sekali\",\n",
    "        \"Pengalaman yang menyenangkan dan tak terlupakan\"\n",
    "    ] * 100\n",
    "    \n",
    "    negative_texts = [\n",
    "        \"Saya kecewa dengan kualitas produk ini\",\n",
    "        \"Film ini membosankan dan tidak menarik\",\n",
    "        \"Pelayanan yang buruk dan tidak profesional\",\n",
    "        \"Makanan di sini tidak enak dan mahal\",\n",
    "        \"Pengalaman yang mengecewakan dan merugikan\"\n",
    "    ] * 100\n",
    "    \n",
    "    neutral_texts = [\n",
    "        \"Produk ini biasa saja tidak istimewa\",\n",
    "        \"Film ini cukup bagus untuk ditonton\",\n",
    "        \"Pelayanan standar seperti tempat lain\",\n",
    "        \"Makanan di sini rasanya biasa saja\",\n",
    "        \"Pengalaman yang cukup normal dan wajar\"\n",
    "    ] * 100\n",
    "    \n",
    "    # Create dataset\n",
    "    texts = positive_texts + negative_texts + neutral_texts\n",
    "    labels = [2] * len(positive_texts) + [0] * len(negative_texts) + [1] * len(neutral_texts)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({'text': texts, 'label': labels})\n",
    "    \n",
    "    # Split into train, val, test\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42, stratify=df['label'])\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Load the dataset\n",
    "train_df, val_df, test_df = load_nusax_sentiment_data()\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(f\"\\nLabel distribution in training set:\")\n",
    "print(train_df['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing and tokenization\n",
    "def preprocess_text(texts, max_features=10000, max_length=100):\n",
    "    \"\"\"\n",
    "    Preprocess text data using TextVectorization\n",
    "    \n",
    "    Parameters:\n",
    "    - texts: list of text strings\n",
    "    - max_features: maximum vocabulary size\n",
    "    - max_length: maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "    - vectorizer: fitted TextVectorization layer\n",
    "    - sequences: tokenized sequences\n",
    "    \"\"\"\n",
    "    # Create TextVectorization layer\n",
    "    vectorizer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=max_features,\n",
    "        output_sequence_length=max_length,\n",
    "        output_mode='int'\n",
    "    )\n",
    "    \n",
    "    # Adapt vectorizer to training texts\n",
    "    vectorizer.adapt(texts)\n",
    "    \n",
    "    # Transform texts to sequences\n",
    "    sequences = vectorizer(texts)\n",
    "    \n",
    "    return vectorizer, sequences\n",
    "\n",
    "# Preprocess the data\n",
    "MAX_FEATURES = 10000\n",
    "MAX_LENGTH = 100\n",
    "\n",
    "# Fit vectorizer on training data and transform all sets\n",
    "vectorizer, train_sequences = preprocess_text(train_df['text'].values, MAX_FEATURES, MAX_LENGTH)\n",
    "_, val_sequences = preprocess_text(val_df['text'].values, MAX_FEATURES, MAX_LENGTH)\n",
    "_, test_sequences = preprocess_text(test_df['text'].values, MAX_FEATURES, MAX_LENGTH)\n",
    "\n",
    "# Use the same vectorizer for all sets\n",
    "val_sequences = vectorizer(val_df['text'].values)\n",
    "test_sequences = vectorizer(test_df['text'].values)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_sequences = train_sequences.numpy()\n",
    "val_sequences = val_sequences.numpy()\n",
    "test_sequences = test_sequences.numpy()\n",
    "\n",
    "train_labels = train_df['label'].values\n",
    "val_labels = val_df['label'].values\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "# Get vocabulary info\n",
    "vocab_size = len(vectorizer.get_vocabulary())\n",
    "num_classes = len(np.unique(train_labels))\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Sequence length: {MAX_LENGTH}\")\n",
    "print(f\"Training sequences shape: {train_sequences.shape}\")\n",
    "print(f\"Training labels shape: {train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Building Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(lstm_layers=1, units_per_layer=[64], bidirectional=False, \n",
    "                     embedding_dim=128, dropout_rate=0.3, vocab_size=10000, \n",
    "                     num_classes=3, sequence_length=100):\n",
    "    \"\"\"\n",
    "    Build LSTM model with specified configuration\n",
    "    \n",
    "    Parameters:\n",
    "    - lstm_layers: number of LSTM layers\n",
    "    - units_per_layer: list of units for each LSTM layer\n",
    "    - bidirectional: whether to use bidirectional LSTM\n",
    "    - embedding_dim: embedding dimension\n",
    "    - dropout_rate: dropout rate\n",
    "    - vocab_size: vocabulary size\n",
    "    - num_classes: number of output classes\n",
    "    - sequence_length: input sequence length\n",
    "    \n",
    "    Returns:\n",
    "    - model: compiled Keras model\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Embedding layer\n",
    "    model.add(tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=sequence_length,\n",
    "        name='embedding'\n",
    "    ))\n",
    "    \n",
    "    # LSTM layers\n",
    "    for i in range(lstm_layers):\n",
    "        # Return sequences for all layers except the last one\n",
    "        return_sequences = i < lstm_layers - 1\n",
    "        \n",
    "        if bidirectional:\n",
    "            model.add(tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.LSTM(\n",
    "                    units_per_layer[i], \n",
    "                    return_sequences=return_sequences,\n",
    "                    name=f'lstm_{i+1}'\n",
    "                ),\n",
    "                name=f'bidirectional_{i+1}'\n",
    "            ))\n",
    "        else:\n",
    "            model.add(tf.keras.layers.LSTM(\n",
    "                units_per_layer[i], \n",
    "                return_sequences=return_sequences,\n",
    "                name=f'lstm_{i+1}'\n",
    "            ))\n",
    "        \n",
    "        # Add dropout after each LSTM layer\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate, name=f'dropout_{i+1}'))\n",
    "    \n",
    "    # Dense output layer\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax', name='dense_output'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_model(model, train_sequences, train_labels, val_sequences, val_labels,\n",
    "                           test_sequences, test_labels, epochs=10, batch_size=32, verbose=1):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model\n",
    "    \n",
    "    Returns:\n",
    "    - history: training history\n",
    "    - test_f1: macro F1 score on test set\n",
    "    - predictions: test predictions\n",
    "    \"\"\"\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_sequences, train_labels,\n",
    "        validation_data=(val_sequences, val_labels),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    test_predictions = model.predict(test_sequences, verbose=0)\n",
    "    test_pred_labels = np.argmax(test_predictions, axis=1)\n",
    "    \n",
    "    # Calculate macro F1 score\n",
    "    test_f1 = f1_score(test_labels, test_pred_labels, average='macro')\n",
    "    \n",
    "    return history, test_f1, test_pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment 1: Effect of Number of LSTM Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Effect of number of LSTM layers\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 1: EFFECT OF NUMBER OF LSTM LAYERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "layer_configs = [\n",
    "    {'layers': 1, 'units': [64], 'name': '1 Layer'},\n",
    "    {'layers': 2, 'units': [64, 32], 'name': '2 Layers'},\n",
    "    {'layers': 3, 'units': [64, 32, 16], 'name': '3 Layers'}\n",
    "]\n",
    "\n",
    "experiment1_results = {}\n",
    "\n",
    "for config in layer_configs:\n",
    "    print(f\"\\nTraining model with {config['name']} ({config['units']})...\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_lstm_model(\n",
    "        lstm_layers=config['layers'],\n",
    "        units_per_layer=config['units'],\n",
    "        bidirectional=False,\n",
    "        vocab_size=vocab_size,\n",
    "        num_classes=num_classes,\n",
    "        sequence_length=MAX_LENGTH\n",
    "    )\n",
    "    \n",
    "    print(f\"Model parameters: {model.count_params():,}\")\n",
    "    \n",
    "    # Train and evaluate\n",
    "    history, test_f1, predictions = train_and_evaluate_model(\n",
    "        model, train_sequences, train_labels, val_sequences, val_labels,\n",
    "        test_sequences, test_labels, epochs=10, verbose=0\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    experiment1_results[config['name']] = {\n",
    "        'history': history,\n",
    "        'test_f1': test_f1,\n",
    "        'predictions': predictions,\n",
    "        'model': model,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    print(f\"Test Macro F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EXPERIMENT 1 RESULTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "for name, result in experiment1_results.items():\n",
    "    print(f\"{name}: Macro F1 = {result['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for Experiment 1\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Experiment 1: Effect of Number of LSTM Layers', fontsize=16)\n",
    "\n",
    "# Training Loss\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "for name, result in experiment1_results.items():\n",
    "    axes[0, 0].plot(result['history'].history['loss'], label=name)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Validation Loss\n",
    "axes[0, 1].set_title('Validation Loss')\n",
    "for name, result in experiment1_results.items():\n",
    "    axes[0, 1].plot(result['history'].history['val_loss'], label=name)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Training Accuracy\n",
    "axes[1, 0].set_title('Training Accuracy')\n",
    "for name, result in experiment1_results.items():\n",
    "    axes[1, 0].plot(result['history'].history['accuracy'], label=name)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Macro F1 Scores\n",
    "names = list(experiment1_results.keys())\n",
    "f1_scores = [experiment1_results[name]['test_f1'] for name in names]\n",
    "axes[1, 1].bar(names, f1_scores)\n",
    "axes[1, 1].set_title('Test Macro F1 Scores')\n",
    "axes[1, 1].set_ylabel('F1 Score')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment 2: Effect of LSTM Units per Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Effect of LSTM units per layer\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 2: EFFECT OF LSTM UNITS PER LAYER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "units_configs = [\n",
    "    {'units': [32, 16], 'name': 'Small (32, 16)'},\n",
    "    {'units': [64, 32], 'name': 'Medium (64, 32)'},\n",
    "    {'units': [128, 64], 'name': 'Large (128, 64)'}\n",
    "]\n",
    "\n",
    "experiment2_results = {}\n",
    "\n",
    "for config in units_configs:\n",
    "    print(f\"\\nTraining model with {config['name']} units...\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_lstm_model(\n",
    "        lstm_layers=2,\n",
    "        units_per_layer=config['units'],\n",
    "        bidirectional=False,\n",
    "        vocab_size=vocab_size,\n",
    "        num_classes=num_classes,\n",
    "        sequence_length=MAX_LENGTH\n",
    "    )\n",
    "    \n",
    "    print(f\"Model parameters: {model.count_params():,}\")\n",
    "    \n",
    "    # Train and evaluate\n",
    "    history, test_f1, predictions = train_and_evaluate_model(\n",
    "        model, train_sequences, train_labels, val_sequences, val_labels,\n",
    "        test_sequences, test_labels, epochs=10, verbose=0\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    experiment2_results[config['name']] = {\n",
    "        'history': history,\n",
    "        'test_f1': test_f1,\n",
    "        'predictions': predictions,\n",
    "        'model': model,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    print(f\"Test Macro F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EXPERIMENT 2 RESULTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "for name, result in experiment2_results.items():\n",
    "    print(f\"{name}: Macro F1 = {result['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for Experiment 2\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Experiment 2: Effect of LSTM Units per Layer', fontsize=16)\n",
    "\n",
    "# Training Loss\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "for name, result in experiment2_results.items():\n",
    "    axes[0, 0].plot(result['history'].history['loss'], label=name)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Validation Loss\n",
    "axes[0, 1].set_title('Validation Loss')\n",
    "for name, result in experiment2_results.items():\n",
    "    axes[0, 1].plot(result['history'].history['val_loss'], label=name)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Training Accuracy\n",
    "axes[1, 0].set_title('Training Accuracy')\n",
    "for name, result in experiment2_results.items():\n",
    "    axes[1, 0].plot(result['history'].history['accuracy'], label=name)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Macro F1 Scores\n",
    "names = list(experiment2_results.keys())\n",
    "f1_scores = [experiment2_results[name]['test_f1'] for name in names]\n",
    "axes[1, 1].bar(names, f1_scores)\n",
    "axes[1, 1].set_title('Test Macro F1 Scores')\n",
    "axes[1, 1].set_ylabel('F1 Score')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment 3: Effect of LSTM Direction (Bidirectional vs Unidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Effect of LSTM direction\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 3: EFFECT OF LSTM DIRECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "direction_configs = [\n",
    "    {'bidirectional': False, 'name': 'Unidirectional'},\n",
    "    {'bidirectional': True, 'name': 'Bidirectional'}\n",
    "]\n",
    "\n",
    "experiment3_results = {}\n",
    "\n",
    "for config in direction_configs:\n",
    "    print(f\"\\nTraining {config['name']} LSTM model...\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_lstm_model(\n",
    "        lstm_layers=2,\n",
    "        units_per_layer=[64, 32],\n",
    "        bidirectional=config['bidirectional'],\n",
    "        vocab_size=vocab_size,\n",
    "        num_classes=num_classes,\n",
    "        sequence_length=MAX_LENGTH\n",
    "    )\n",
    "    \n",
    "    print(f\"Model parameters: {model.count_params():,}\")\n",
    "    \n",
    "    # Train and evaluate\n",
    "    history, test_f1, predictions = train_and_evaluate_model(\n",
    "        model, train_sequences, train_labels, val_sequences, val_labels,\n",
    "        test_sequences, test_labels, epochs=10, verbose=0\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    experiment3_results[config['name']] = {\n",
    "        'history': history,\n",
    "        'test_f1': test_f1,\n",
    "        'predictions': predictions,\n",
    "        'model': model,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    print(f\"Test Macro F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EXPERIMENT 3 RESULTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "for name, result in experiment3_results.items():\n",
    "    print(f\"{name}: Macro F1 = {result['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for Experiment 3\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Experiment 3: Effect of LSTM Direction', fontsize=16)\n",
    "\n",
    "# Training Loss\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "for name, result in experiment3_results.items():\n",
    "    axes[0, 0].plot(result['history'].history['loss'], label=name)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Validation Loss\n",
    "axes[0, 1].set_title('Validation Loss')\n",
    "for name, result in experiment3_results.items():\n",
    "    axes[0, 1].plot(result['history'].history['val_loss'], label=name)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Training Accuracy\n",
    "axes[1, 0].set_title('Training Accuracy')\n",
    "for name, result in experiment3_results.items():\n",
    "    axes[1, 0].plot(result['history'].history['accuracy'], label=name)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Macro F1 Scores\n",
    "names = list(experiment3_results.keys())\n",
    "f1_scores = [experiment3_results[name]['test_f1'] for name in names]\n",
    "axes[1, 1].bar(names, f1_scores)\n",
    "axes[1, 1].set_title('Test Macro F1 Scores')\n",
    "axes[1, 1].set_ylabel('F1 Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Select Best Model and Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model across all experiments\n",
    "all_results = {}\n",
    "all_results.update(experiment1_results)\n",
    "all_results.update(experiment2_results)\n",
    "all_results.update(experiment3_results)\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(all_results.keys(), key=lambda x: all_results[x]['test_f1'])\n",
    "best_result = all_results[best_model_name]\n",
    "best_model = best_result['model']\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BEST MODEL SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best F1 Score: {best_result['test_f1']:.4f}\")\n",
    "print(f\"Model Parameters: {best_model.count_params():,}\")\n",
    "\n",
    "# Save the best model weights\n",
    "best_model.save_weights('lstm_keras_best.weights.h5')\n",
    "print(\"\\nBest model weights saved to 'lstm_keras_best.weights.h5'\")\n",
    "\n",
    "# Save model configuration for from-scratch implementation\n",
    "if 'config' in best_result:\n",
    "    best_config = best_result['config'].copy()\n",
    "else:\n",
    "    # Default config if not found\n",
    "    best_config = {\n",
    "        'lstm_layers': 2,\n",
    "        'units_per_layer': [64, 32],\n",
    "        'bidirectional': False,\n",
    "        'embedding_dim': 128\n",
    "    }\n",
    "\n",
    "# Add additional config info\n",
    "best_config.update({\n",
    "    'vocab_size': vocab_size,\n",
    "    'num_classes': num_classes,\n",
    "    'sequence_length': MAX_LENGTH,\n",
    "    'final_f1_score': best_result['test_f1']\n",
    "})\n",
    "\n",
    "print(\"\\nBest model configuration:\")\n",
    "for key, value in best_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for from-scratch testing\n",
    "data_to_save = {\n",
    "    'test_sequences': test_sequences,\n",
    "    'test_labels': test_labels,\n",
    "    'best_config': best_config,\n",
    "    'vocab_size': vocab_size,\n",
    "}\n",
    "\n",
    "with open('lstm_saved_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data_to_save, f)\n",
    "\n",
    "print(\"Data successfully saved to lstm_saved_data.pkl\")\n",
    "print(f\"Saved test_sequences shape: {test_sequences.shape}\")\n",
    "print(f\"Saved test_labels shape: {test_labels.shape}\")\n",
    "print(f\"Saved best_config: {best_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed analysis of the best model\n",
    "print(\"=\" * 60)\n",
    "print(\"DETAILED BEST MODEL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nBest Model Architecture:\")\n",
    "best_model.summary()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, best_result['predictions']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, best_result['predictions'])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(best_result['history'].history['loss'], label='Training Loss')\n",
    "axes[0].plot(best_result['history'].history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_title(f'Training History - {best_model_name}')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(best_result['history'].history['accuracy'], label='Training Accuracy')\n",
    "axes[1].plot(best_result['history'].history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[1].set_title(f'Training History - {best_model_name}')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. From-Scratch Implementation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from-scratch implementation\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "try:\n",
    "    from from_scratch.model import LSTMModelFromScratch\n",
    "    from from_scratch.layers import (\n",
    "        Embedding, LSTM, Bidirectional, Dropout, \n",
    "        Dense, Softmax\n",
    "    )\n",
    "    print(\"Successfully imported from-scratch modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing from-scratch modules: {e}\")\n",
    "    print(\"Make sure the from_scratch directory is in the correct path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create from-scratch model using the best Keras model weights\n",
    "def create_from_scratch_model(keras_model):\n",
    "    \"\"\"\n",
    "    Create a from-scratch model based on the Keras model weights\n",
    "    \"\"\"\n",
    "    layers_from_scratch = []\n",
    "    \n",
    "    # Extract weights from each layer of the Keras model\n",
    "    for i, layer in enumerate(keras_model.layers):\n",
    "        if isinstance(layer, tf.keras.layers.Embedding):\n",
    "            # Extract embedding weights\n",
    "            weights = layer.get_weights()[0]\n",
    "            layers_from_scratch.append(Embedding(weights))\n",
    "            print(f\"Added Embedding layer: {weights.shape}\")\n",
    "            \n",
    "        elif isinstance(layer, tf.keras.layers.LSTM):\n",
    "            # Extract LSTM weights: kernel, recurrent, bias\n",
    "            weights = layer.get_weights()\n",
    "            if len(weights) == 3:\n",
    "                kernel, recurrent, bias = weights\n",
    "                layers_from_scratch.append(LSTM(kernel, recurrent, bias, layer.return_sequences))\n",
    "                print(f\"Added LSTM layer: kernel{kernel.shape}, recurrent{recurrent.shape}\")\n",
    "            \n",
    "        elif isinstance(layer, tf.keras.layers.Bidirectional):\n",
    "            # Extract bidirectional LSTM weights\n",
    "            weights = layer.get_weights()\n",
    "            if len(weights) == 6:  # Two sets of weights for forward and backward\n",
    "                forward_kernel, forward_recurrent, forward_bias = weights[0:3]\n",
    "                backward_kernel, backward_recurrent, backward_bias = weights[3:6]\n",
    "                \n",
    "                forward_lstm = LSTM(forward_kernel, forward_recurrent, forward_bias, layer.layer.return_sequences)\n",
    "                backward_lstm = LSTM(backward_kernel, backward_recurrent, backward_bias, layer.layer.return_sequences)\n",
    "                \n",
    "                layers_from_scratch.append(Bidirectional(forward_lstm, backward_lstm))\n",
    "                print(f\"Added Bidirectional LSTM layer\")\n",
    "            \n",
    "        elif isinstance(layer, tf.keras.layers.Dropout):\n",
    "            layers_from_scratch.append(Dropout(layer.rate))\n",
    "            print(f\"Added Dropout layer: rate={layer.rate}\")\n",
    "            \n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            # Extract dense weights and bias\n",
    "            weights, bias = layer.get_weights()\n",
    "            layers_from_scratch.append(Dense(weights, bias))\n",
    "            print(f\"Added Dense layer: {weights.shape}\")\n",
    "            \n",
    "            # Add softmax activation if this is the output layer\n",
    "            if i == len(keras_model.layers) - 1:\n",
    "                layers_from_scratch.append(Softmax())\n",
    "                print(\"Added Softmax activation\")\n",
    "    \n",
    "    return LSTMModelFromScratch(layers_from_scratch)\n",
    "\n",
    "# Create from-scratch model\n",
    "print(\"Creating from-scratch model...\")\n",
    "from_scratch_model = create_from_scratch_model(best_model)\n",
    "\n",
    "# Display model summary\n",
    "print(\"\\nFrom-scratch model summary:\")\n",
    "from_scratch_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Keras and from-scratch predictions\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARING KERAS VS FROM-SCRATCH IMPLEMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Make predictions using both models\n",
    "print(\"Making predictions with Keras model...\")\n",
    "keras_predictions = np.argmax(best_model.predict(test_sequences, verbose=0), axis=1)\n",
    "\n",
    "print(\"Making predictions with from-scratch model...\")\n",
    "from_scratch_predictions = from_scratch_model.predict(test_sequences)\n",
    "\n",
    "# Calculate F1 scores\n",
    "keras_f1 = f1_score(test_labels, keras_predictions, average='macro')\n",
    "from_scratch_f1 = f1_score(test_labels, from_scratch_predictions, average='macro')\n",
    "\n",
    "print(f\"\\nResults comparison:\")\n",
    "print(f\"Keras model macro F1 score: {keras_f1:.4f}\")\n",
    "print(f\"From-scratch model macro F1 score: {from_scratch_f1:.4f}\")\n",
    "print(f\"Difference: {abs(keras_f1 - from_scratch_f1):.4f}\")\n",
    "\n",
    "# Check how many predictions match between the two models\n",
    "matches = np.sum(keras_predictions == from_scratch_predictions)\n",
    "match_percentage = (matches / len(keras_predictions)) * 100\n",
    "\n",
    "print(f\"\\nPrediction agreement: {match_percentage:.2f}%\")\n",
    "print(f\"Matching predictions: {matches}/{len(keras_predictions)}\")\n",
    "\n",
    "# Compare a few example predictions\n",
    "print(\"\\nSample prediction comparison:\")\n",
    "for i in range(min(10, len(test_sequences))):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  True label: {test_labels[i]}\")\n",
    "    print(f\"  Keras prediction: {keras_predictions[i]}\")\n",
    "    print(f\"  From-scratch prediction: {from_scratch_predictions[i]}\")\n",
    "    print(f\"  Match: {'✓' if keras_predictions[i] == from_scratch_predictions[i] else '✗'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analysis and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive analysis and conclusions\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE ANALYSIS AND CONCLUSIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. EFFECT OF NUMBER OF LSTM LAYERS:\")\n",
    "print(\"-\" * 50)\n",
    "for name, result in experiment1_results.items():\n",
    "    print(f\"{name}: F1 = {result['test_f1']:.4f}, Params = {result['model'].count_params():,}\")\n",
    "\n",
    "best_layers = max(experiment1_results.keys(), key=lambda x: experiment1_results[x]['test_f1'])\n",
    "print(f\"\\nConclusion: {best_layers} performed best among layer configurations.\")\n",
    "print(\"Deep networks can capture more complex patterns but may also overfit with limited data.\")\n",
    "\n",
    "print(\"\\n2. EFFECT OF LSTM UNITS PER LAYER:\")\n",
    "print(\"-\" * 50)\n",
    "for name, result in experiment2_results.items():\n",
    "    print(f\"{name}: F1 = {result['test_f1']:.4f}, Params = {result['model'].count_params():,}\")\n",
    "\n",
    "best_units = max(experiment2_results.keys(), key=lambda x: experiment2_results[x]['test_f1'])\n",
    "print(f\"\\nConclusion: {best_units} configuration performed best.\")\n",
    "print(\"More units can capture more complex representations but require more data and training time.\")\n",
    "\n",
    "print(\"\\n3. EFFECT OF LSTM DIRECTION:\")\n",
    "print(\"-\" * 50)\n",
    "for name, result in experiment3_results.items():\n",
    "    print(f\"{name}: F1 = {result['test_f1']:.4f}, Params = {result['model'].count_params():,}\")\n",
    "\n",
    "best_direction = max(experiment3_results.keys(), key=lambda x: experiment3_results[x]['test_f1'])\n",
    "print(f\"\\nConclusion: {best_direction} LSTM performed best.\")\n",
    "print(\"Bidirectional LSTMs can capture dependencies from both directions but double the parameters.\")\n",
    "\n",
    "print(\"\\n4. FROM-SCRATCH IMPLEMENTATION:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Implementation accuracy: {match_percentage:.2f}% agreement with Keras\")\n",
    "print(f\"F1 score difference: {abs(keras_f1 - from_scratch_f1):.4f}\")\n",
    "print(\"\\nConclusion: The from-scratch implementation successfully replicates Keras behavior.\")\n",
    "print(\"Small differences may be due to numerical precision or implementation details.\")\n",
    "\n",
    "print(\"\\n5. OVERALL RECOMMENDATIONS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"• Best overall model: {best_model_name}\")\n",
    "print(f\"• Best F1 score: {best_result['test_f1']:.4f}\")\n",
    "print(f\"• Model complexity: {best_model.count_params():,} parameters\")\n",
    "print(\"• The from-scratch implementation demonstrates understanding of LSTM internals\")\n",
    "print(\"• Both unidirectional and bidirectional LSTMs can be effective for text classification\")\n",
    "print(\"• Model selection should balance performance and computational efficiency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results summary\n",
    "final_results = {\n",
    "    'experiment1_results': {name: {'test_f1': result['test_f1'], 'config': result['config']} \n",
    "                           for name, result in experiment1_results.items()},\n",
    "    'experiment2_results': {name: {'test_f1': result['test_f1'], 'config': result['config']} \n",
    "                           for name, result in experiment2_results.items()},\n",
    "    'experiment3_results': {name: {'test_f1': result['test_f1'], 'config': result['config']} \n",
    "                           for name, result in experiment3_results.items()},\n",
    "    'best_model': {\n",
    "        'name': best_model_name,\n",
    "        'f1_score': best_result['test_f1'],\n",
    "        'config': best_config\n",
    "    },\n",
    "    'from_scratch_comparison': {\n",
    "        'keras_f1': keras_f1,\n",
    "        'from_scratch_f1': from_scratch_f1,\n",
    "        'agreement_percentage': match_percentage\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to JSON file\n",
    "with open('lstm_experiment_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(\"Final results saved to 'lstm_experiment_results.json'\")\n",
    "print(\"\\nExperiment completed successfully!\")\n",
    "print(\"\\nFiles generated:\")\n",
    "print(\"- lstm_keras_best.weights.h5: Best model weights\")\n",
    "print(\"- lstm_saved_data.pkl: Test data and configuration\")\n",
    "print(\"- lstm_experiment_results.json: Experiment results summary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}