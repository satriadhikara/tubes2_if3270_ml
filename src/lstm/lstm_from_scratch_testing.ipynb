{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66d0897",
   "metadata": {},
   "source": [
    "# LSTM From Scratch Testing\n",
    "This notebook tests the LSTM \"from scratch\" implementation by comparing it with the Keras implementation. We'll compare the predictions from both implementations and verify that they produce similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e2b48",
   "metadata": {},
   "source": [
    "## How to Run This Notebook\n",
    "\n",
    "1. **First, run the `lstm_keras_training.ipynb` notebook** to train the LSTM model and save the weights\n",
    "\n",
    "2. After training is complete, the last cell in `lstm_keras_training.ipynb` should save data to `lstm_saved_data.pkl`\n",
    "\n",
    "3. Make sure the following files exist:\n",
    "   - `lstm_keras_best.weights.h5`: The saved weights from the best model\n",
    "   - `lstm_saved_data.pkl`: Saved test data and model configuration\n",
    "   \n",
    "4. This notebook will:\n",
    "   - Load the saved model configuration and weights\n",
    "   - Create a from-scratch implementation using the same weights\n",
    "   - Compare the predictions from both implementations\n",
    "   - Calculate the agreement percentage between both models\n",
    "   \n",
    "If the saved data is not found, this notebook will create synthetic data for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37ebeba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\User\\Documents\\Semester6\\ML\\tubes2_if3270_ml\\src\\lstm\n",
      "Successfully imported from_scratch modules\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup path to access modules\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Install any missing packages if needed\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError:\n",
    "    print(\"Installing TensorFlow...\")\n",
    "    !pip install tensorflow\n",
    "\n",
    "# Import our from_scratch implementation\n",
    "try:\n",
    "    from lstm.from_scratch.model import LSTMModelFromScratch\n",
    "    from lstm.from_scratch.layers import (\n",
    "        Embedding, LSTM, Bidirectional, Dropout, \n",
    "        Dense, Softmax\n",
    "    )\n",
    "    print(\"Successfully imported from_scratch modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing from_scratch modules: {e}\")\n",
    "    print(\"Check that the path is correct and __init__.py exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to create synthetic data if needed\n",
    "def create_synthetic_data(vocab_size=1000, seq_length=100, num_samples=100, num_classes=5):\n",
    "    \"\"\"\n",
    "    Create synthetic text classification data for testing\n",
    "    \"\"\"\n",
    "    # Generate random token sequences\n",
    "    sequences = np.random.randint(1, vocab_size, size=(num_samples, seq_length))\n",
    "    # Generate random labels\n",
    "    labels = np.random.randint(0, num_classes, size=num_samples)\n",
    "    \n",
    "    # Create simplified model config\n",
    "    config = {\n",
    "        'lstm_layers': 2,\n",
    "        'units_per_layer': [64, 64],\n",
    "        'bidirectional': False,\n",
    "        'embedding_dim': 32\n",
    "    }\n",
    "    \n",
    "    return sequences, labels, config, vocab_size\n",
    "\n",
    "# Load the trained Keras model and test data\n",
    "def load_model_and_data():\n",
    "    \"\"\"\n",
    "    Load the best Keras model and the test data\n",
    "    \"\"\"\n",
    "    # Find the model weights file\n",
    "    possible_weight_paths = [\n",
    "        \"lstm_keras_best.weights.h5\",  # Current directory\n",
    "        \"../lstm_keras_best.weights.h5\",  # One level up\n",
    "        \"../lstm/lstm_keras_best.weights.h5\",  # In lstm directory  \n",
    "    ]\n",
    "    \n",
    "    weights_path = None\n",
    "    for path in possible_weight_paths:\n",
    "        if os.path.exists(path):\n",
    "            weights_path = path\n",
    "            print(f\"Found weights file: {weights_path}\")\n",
    "            break\n",
    "    \n",
    "    # Look for saved data pickle file\n",
    "    possible_data_paths = [\n",
    "        \"lstm_saved_data.pkl\",  # Current directory\n",
    "        \"../lstm_saved_data.pkl\",  # One level up\n",
    "        \"../lstm/lstm_saved_data.pkl\",  # In lstm directory\n",
    "    ]\n",
    "    \n",
    "    data_path = None\n",
    "    for path in possible_data_paths:\n",
    "        if os.path.exists(path):\n",
    "            data_path = path\n",
    "            print(f\"Found saved data: {data_path}\")\n",
    "            break\n",
    "    \n",
    "    try:\n",
    "        # If we found a data file, load it\n",
    "        if data_path:\n",
    "            with open(data_path, 'rb') as f:\n",
    "                saved_data = pickle.load(f)\n",
    "            \n",
    "            test_sequences = saved_data['test_sequences']\n",
    "            test_labels = saved_data['test_labels']\n",
    "            best_config = saved_data['best_config']\n",
    "            vocab_size = saved_data['vocab_size']\n",
    "            \n",
    "            print(f\"Loaded saved data: {len(test_sequences)} test sequences, vocab_size={vocab_size}\")\n",
    "        else:\n",
    "            # If no data file, create synthetic data\n",
    "            print(\"No saved data found. Creating synthetic data for testing...\")\n",
    "            test_sequences, test_labels, best_config, vocab_size = create_synthetic_data()\n",
    "            print(\"Using synthetic data:\", test_sequences.shape, test_labels.shape)\n",
    "        \n",
    "        # Define build_lstm_model function locally\n",
    "        def build_lstm_model(lstm_layers, units_per_layer, bidirectional, vocab_size,\n",
    "                           embedding_dim=128, num_classes=5, sequence_length=100):\n",
    "            \"\"\"\n",
    "            Create a simple LSTM model with the specified configuration\n",
    "            \"\"\"\n",
    "            model = tf.keras.Sequential()\n",
    "            \n",
    "            # Add embedding layer\n",
    "            model.add(tf.keras.layers.Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=embedding_dim,\n",
    "                input_length=sequence_length\n",
    "            ))\n",
    "            \n",
    "            # Add LSTM layers\n",
    "            for i in range(lstm_layers):\n",
    "                return_sequences = i < lstm_layers - 1\n",
    "                \n",
    "                if bidirectional:\n",
    "                    model.add(tf.keras.layers.Bidirectional(\n",
    "                        tf.keras.layers.LSTM(units_per_layer[i], return_sequences=return_sequences)\n",
    "                    ))\n",
    "                else:\n",
    "                    model.add(tf.keras.layers.LSTM(units_per_layer[i], return_sequences=return_sequences))\n",
    "                \n",
    "                # Add dropout after each LSTM layer\n",
    "                model.add(tf.keras.layers.Dropout(0.3))\n",
    "            \n",
    "            # Add Dense output layer\n",
    "            model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "            \n",
    "            # Compile the model\n",
    "            model.compile(\n",
    "                optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            return model\n",
    "            \n",
    "        # Build the Keras model\n",
    "        sequence_length = test_sequences.shape[1] if hasattr(test_sequences, 'shape') else 100\n",
    "        num_classes = len(np.unique(test_labels)) if hasattr(test_labels, '__len__') else 5\n",
    "        embedding_dim = best_config.get('embedding_dim', 128)\n",
    "        \n",
    "        print(f\"Building model with: vocab_size={vocab_size}, sequence_length={sequence_length}, num_classes={num_classes}\")\n",
    "        model = build_lstm_model(\n",
    "            lstm_layers=best_config['lstm_layers'],\n",
    "            units_per_layer=best_config['units_per_layer'],\n",
    "            bidirectional=best_config.get('bidirectional', False),\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_classes=num_classes,\n",
    "            sequence_length=sequence_length\n",
    "        )\n",
    "        \n",
    "        # Build the model to initialize parameters properly\n",
    "        # Create a sample input to pass through the model\n",
    "        if hasattr(test_sequences, 'shape'):\n",
    "            sample_batch = np.zeros((1, sequence_length), dtype=np.int32)\n",
    "            # Do a forward pass to initialize the model's parameters\n",
    "            _ = model(sample_batch)\n",
    "        \n",
    "        # Load the weights if available\n",
    "        if weights_path and os.path.exists(weights_path):\n",
    "            print(f\"Loading weights from {weights_path}\")\n",
    "            try:\n",
    "                model.load_weights(weights_path)\n",
    "                print(\"Weights loaded successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading weights: {e}\")\n",
    "                print(\"Will continue with randomly initialized weights\")\n",
    "        else:\n",
    "            print(\"No weights file found, using randomly initialized weights\")\n",
    "        \n",
    "        return model, test_sequences, test_labels\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in load_model_and_data: {e}\")\n",
    "        print(\"Creating synthetic data as a fallback...\")\n",
    "        \n",
    "        # Fall back to synthetic data if anything goes wrong\n",
    "        test_sequences, test_labels, best_config, vocab_size = create_synthetic_data()\n",
    "        \n",
    "        model = build_lstm_model(\n",
    "            lstm_layers=best_config['lstm_layers'], \n",
    "            units_per_layer=best_config['units_per_layer'],\n",
    "            bidirectional=best_config['bidirectional'], \n",
    "            vocab_size=vocab_size\n",
    "        )\n",
    "        \n",
    "        return model, test_sequences, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12720359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create from scratch model\n",
    "def create_from_scratch_model(keras_model):\n",
    "    \"\"\"\n",
    "    Create a from_scratch model based on the Keras model\n",
    "    \"\"\"\n",
    "    from lstm.from_scratch.layers import Embedding, LSTM, Bidirectional, Dropout, Dense, Softmax\n",
    "    \n",
    "    layers_from_scratch = []\n",
    "    \n",
    "    # Extract weights from each layer of the Keras model\n",
    "    for i, layer in enumerate(keras_model.layers):\n",
    "        if isinstance(layer, keras.layers.Embedding):\n",
    "            # Extract embedding weights\n",
    "            weights = layer.get_weights()[0]\n",
    "            layers_from_scratch.append(Embedding(weights))\n",
    "            \n",
    "        elif isinstance(layer, keras.layers.LSTM):\n",
    "            # Extract LSTM weights: kernel, recurrent, bias\n",
    "            weights = layer.get_weights()\n",
    "            if len(weights) == 3:\n",
    "                kernel, recurrent, bias = weights\n",
    "                layers_from_scratch.append(LSTM(kernel, recurrent, bias, layer.return_sequences))\n",
    "            \n",
    "        elif isinstance(layer, keras.layers.Bidirectional):\n",
    "            # Extract bidirectional LSTM weights\n",
    "            weights = layer.get_weights()\n",
    "            if len(weights) == 6:  # Two sets of weights for forward and backward\n",
    "                forward_kernel, forward_recurrent, forward_bias = weights[0:3]\n",
    "                backward_kernel, backward_recurrent, backward_bias = weights[3:6]\n",
    "                \n",
    "                forward_lstm = LSTM(forward_kernel, forward_recurrent, forward_bias, layer.layer.return_sequences)\n",
    "                backward_lstm = LSTM(backward_kernel, backward_recurrent, backward_bias, layer.layer.return_sequences)\n",
    "                \n",
    "                layers_from_scratch.append(Bidirectional(forward_lstm, backward_lstm))\n",
    "            \n",
    "        elif isinstance(layer, keras.layers.Dropout):\n",
    "            layers_from_scratch.append(Dropout(layer.rate))\n",
    "            \n",
    "        elif isinstance(layer, keras.layers.Dense):\n",
    "            # Extract dense weights and bias\n",
    "            weights, bias = layer.get_weights()\n",
    "            layers_from_scratch.append(Dense(weights, bias))\n",
    "            \n",
    "            # Add softmax activation if this is the output layer\n",
    "            if i == len(keras_model.layers) - 1:\n",
    "                layers_from_scratch.append(Softmax())\n",
    "    \n",
    "    return LSTMModelFromScratch(layers_from_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878549f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Keras model and test data...\n",
      "No saved data found. Creating synthetic data for testing...\n",
      "Using synthetic data: (100, 100) (100,)\n",
      "Building model with: vocab_size=1000, sequence_length=100, num_classes=5\n",
      "No weights file found, using randomly initialized weights\n",
      "\n",
      "Creating from-scratch model...\n",
      "Error during model comparison: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\Semester6\\ML\\tubes2_if3270_ml\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the Keras model and test data\n",
    "print(\"Loading Keras model and test data...\")\n",
    "keras_model, test_sequences, test_labels = load_model_and_data()\n",
    "\n",
    "# Sanity check the loaded data\n",
    "if keras_model is not None and test_sequences is not None:\n",
    "    try:\n",
    "        # Make sure the Keras model is built by doing a forward pass on a small sample batch\n",
    "        sample_batch = test_sequences[:1]\n",
    "        keras_model(sample_batch)  # Forward pass to build the model\n",
    "        \n",
    "        # Now print the summary after the model is built\n",
    "        print(\"\\nKeras model summary (with built parameters):\")\n",
    "        keras_model.summary()\n",
    "        \n",
    "        # Create the from-scratch model\n",
    "        print(\"\\nCreating from-scratch model...\")\n",
    "        from_scratch_model = create_from_scratch_model(keras_model)\n",
    "        \n",
    "        # Display the model summary\n",
    "        print(\"\\nFrom-scratch model summary:\")\n",
    "        from_scratch_model.summary()\n",
    "        \n",
    "        # Make predictions using both models\n",
    "        print(\"\\nMaking predictions with Keras model...\")\n",
    "        keras_predictions = np.argmax(keras_model.predict(test_sequences), axis=1)\n",
    "        \n",
    "        print(\"Making predictions with from-scratch model...\")\n",
    "        from_scratch_predictions = from_scratch_model.predict(test_sequences)\n",
    "        \n",
    "        # Calculate F1 scores\n",
    "        keras_f1 = f1_score(test_labels, keras_predictions, average='macro')\n",
    "        from_scratch_f1 = f1_score(test_labels, from_scratch_predictions, average='macro')\n",
    "        \n",
    "        print(f\"\\nResults comparison:\")\n",
    "        print(f\"Keras model macro F1 score: {keras_f1:.4f}\")\n",
    "        print(f\"From-scratch model macro F1 score: {from_scratch_f1:.4f}\")\n",
    "        \n",
    "        # Check how many predictions match between the two models\n",
    "        matches = np.sum(keras_predictions == from_scratch_predictions)\n",
    "        match_percentage = (matches / len(keras_predictions)) * 100\n",
    "        \n",
    "        print(f\"\\nPrediction match between Keras and from-scratch implementation: {match_percentage:.2f}%\")\n",
    "        \n",
    "        # Compare a few example predictions\n",
    "        print(\"\\nSample prediction comparison:\")\n",
    "        for i in range(min(5, len(test_sequences))):\n",
    "            print(f\"Example {i+1}:\")\n",
    "            print(f\"  True label: {test_labels[i]}\")\n",
    "            print(f\"  Keras prediction: {keras_predictions[i]}\")\n",
    "            print(f\"  From-scratch prediction: {from_scratch_predictions[i]}\")\n",
    "            print()\n",
    "            \n",
    "        # Print detailed parameter breakdown for the from-scratch model\n",
    "        print(\"\\nDetailed parameter count for from-scratch model:\")\n",
    "        \n",
    "        # Count parameters by layer type\n",
    "        embedding_params = 0\n",
    "        lstm_params = 0\n",
    "        dense_params = 0\n",
    "        \n",
    "        for i, layer in enumerate(from_scratch_model.layers):\n",
    "            layer_name = layer.__class__.__name__\n",
    "            layer_params = 0\n",
    "            \n",
    "            if layer_name == 'Embedding':\n",
    "                layer_params = np.size(layer.weights)\n",
    "                embedding_params += layer_params\n",
    "            elif layer_name == 'LSTM':\n",
    "                kernel_params = np.size(layer.cell.weights_kernel)\n",
    "                recurrent_params = np.size(layer.cell.weights_recurrent)\n",
    "                bias_params = np.size(layer.cell.bias)\n",
    "                layer_params = kernel_params + recurrent_params + bias_params\n",
    "                lstm_params += layer_params\n",
    "            elif layer_name == 'Bidirectional':\n",
    "                # Forward LSTM\n",
    "                f_kernel_params = np.size(layer.forward_layer.cell.weights_kernel)\n",
    "                f_recurrent_params = np.size(layer.forward_layer.cell.weights_recurrent)\n",
    "                f_bias_params = np.size(layer.forward_layer.cell.bias)\n",
    "                \n",
    "                # Backward LSTM\n",
    "                b_kernel_params = np.size(layer.backward_layer.cell.weights_kernel)\n",
    "                b_recurrent_params = np.size(layer.backward_layer.cell.weights_recurrent)\n",
    "                b_bias_params = np.size(layer.backward_layer.cell.bias)\n",
    "                \n",
    "                layer_params = f_kernel_params + f_recurrent_params + f_bias_params + \\\n",
    "                               b_kernel_params + b_recurrent_params + b_bias_params\n",
    "                lstm_params += layer_params\n",
    "            elif layer_name == 'Dense':\n",
    "                weights_params = np.size(layer.weights)\n",
    "                bias_params = np.size(layer.bias)\n",
    "                layer_params = weights_params + bias_params\n",
    "                dense_params += layer_params\n",
    "            \n",
    "            if layer_params > 0:\n",
    "                print(f\"  Layer {i+1} ({layer_name}): {layer_params:,} parameters\")\n",
    "        \n",
    "        # Print summary\n",
    "        total_params = embedding_params + lstm_params + dense_params\n",
    "        print(\"\\nParameter summary:\")\n",
    "        print(f\"  Embedding layers: {embedding_params:,} parameters\")\n",
    "        print(f\"  LSTM/Bidirectional layers: {lstm_params:,} parameters\")\n",
    "        print(f\"  Dense layers: {dense_params:,} parameters\")\n",
    "        print(f\"  Total: {total_params:,} parameters\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during model comparison: {e}\")\n",
    "else:\n",
    "    print(\"ERROR: Failed to load model or test data. Check previous error messages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3364f580",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The from-scratch implementation of the LSTM model should produce results very similar to the Keras implementation, as it uses the same weights. Any small differences might be due to:\n",
    "\n",
    "1. Floating-point precision differences in calculations\n",
    "2. Differences in the implementation details of activation functions\n",
    "3. Subtle differences in how operations like matrix multiplication are implemented\n",
    "\n",
    "The implementation successfully demonstrates how LSTM networks work under the hood, including:\n",
    "\n",
    "- Embedding layer for converting token indices to vector representations\n",
    "- LSTM cells with their gating mechanisms (input, forget, output gates)\n",
    "- Bidirectional LSTM implementations that process sequences in both directions\n",
    "- Dense layers for final classification\n",
    "\n",
    "This implementation can be used as a reference to understand how these deep learning components work without relying on high-level frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86579a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing agreement between models...\n",
      "Error creating visualizations: name 'keras_predictions' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Visualize the results\n",
    "try:\n",
    "    # Create a confusion matrix to visualize agreement between models\n",
    "    print(\"Visualizing agreement between models...\")\n",
    "    \n",
    "    # Create a confusion matrix between the two models\n",
    "    cm = confusion_matrix(keras_predictions, from_scratch_predictions)\n",
    "    \n",
    "    # Get number of classes\n",
    "    num_classes = cm.shape[0]\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=range(num_classes),\n",
    "                yticklabels=range(num_classes))\n",
    "    plt.xlabel('From-Scratch Model Predictions')\n",
    "    plt.ylabel('Keras Model Predictions')\n",
    "    plt.title('Agreement Between Keras and From-Scratch Models')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the distribution of predictions\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(x=keras_predictions)\n",
    "    plt.title('Keras Model Predictions')\n",
    "    plt.xlabel('Class')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.countplot(x=from_scratch_predictions)\n",
    "    plt.title('From-Scratch Model Predictions')\n",
    "    plt.xlabel('Class')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating visualizations: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
