{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf53476",
   "metadata": {},
   "source": [
    "# RNN From Scratch Implementation\\n\n",
    "    This notebook demonstrates RNN implementation from scratch using custom layers and compares it with Keras implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0fa84",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ba0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1791489f",
   "metadata": {},
   "source": [
    "## 2. Import From-Scratch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eeee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current directory (notebook folder)\n",
    "current_dir = os.getcwd()\n",
    "# Go up one level to src/rnn, then import from_scratch\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "\n",
    "# Import our from-scratch implementation\n",
    "try:\n",
    "    from from_scratch.layers import (\n",
    "        Embedding, SimpleRNN, Bidirectional, Dropout, \n",
    "        Dense, Softmax, RNNCell\n",
    "    )\n",
    "    from from_scratch.model import RNNModelFromScratch\n",
    "    print(\"âœ… Successfully imported from-scratch modules!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing from-scratch modules: {e}\")\n",
    "    print(\"Make sure the directory structure is correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cfe0ba",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab1061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nusax_sentiment_data():,\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Load NusaX-Sentiment dataset from local CSV files\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        # Define paths to your dataset files\\n\",\n",
    "    \"        data_dir = \\\"../../../data/nusax_sentiment\\\"\\n\",\n",
    "    \"        train_file = os.path.join(data_dir, \\\"train.csv\\\")\\n\",\n",
    "    \"        valid_file = os.path.join(data_dir, \\\"valid.csv\\\")\\n\",\n",
    "    \"        test_file = os.path.join(data_dir, \\\"test.csv\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Load the datasets\\n\",\n",
    "    \"        train_df = pd.read_csv(train_file)\\n\",\n",
    "    \"        valid_df = pd.read_csv(valid_file)\\n\",\n",
    "    \"        test_df = pd.read_csv(test_file)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert string labels to integer codes\\n\",\n",
    "    \"        label_categories = pd.Categorical(train_df['label'].values)\\n\",\n",
    "    \"        train_df['label'] = label_categories.codes\\n\",\n",
    "    \"        label_mapping = dict(enumerate(label_categories.categories))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Use the same mapping for valid and test\\n\",\n",
    "    \"        valid_df['label'] = pd.Categorical(valid_df['label'].values, categories=label_categories.categories).codes\\n\",\n",
    "    \"        test_df['label'] = pd.Categorical(test_df['label'].values, categories=label_categories.categories).codes\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(\\\"Dataset loaded successfully from local files.\\\")\\n\",\n",
    "    \"        print(f\\\"Label mapping: {label_mapping}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return train_df, valid_df, test_df, label_mapping\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    except FileNotFoundError as e:\\n\",\n",
    "    \"        print(f\\\"Error: Could not find dataset files. {e}\\\")\\n\",\n",
    "    \"        print(\\\"Creating synthetic dataset for demonstration...\\\")\\n\",\n",
    "    \"        return create_synthetic_data()\\n\",\n",
    "    \"\\n\",\n",
    "    \"def create_synthetic_data():\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Create synthetic sentiment data for demonstration\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    np.random.seed(42)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Sample texts for each sentiment\\n\",\n",
    "    \"    positive_texts = [\\n\",\n",
    "    \"        \\\"I love this product\\\", \\\"Great quality\\\", \\\"Excellent service\\\", \\\"Amazing experience\\\",\\n\",\n",
    "    \"        \\\"Perfect solution\\\", \\\"Highly recommend\\\", \\\"Outstanding quality\\\", \\\"Very satisfied\\\"\\n\",\n",
    "    \"    ] * 100\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    negative_texts = [\\n\",\n",
    "    \"        \\\"Poor quality\\\", \\\"Terrible service\\\", \\\"Disappointing product\\\", \\\"Waste of money\\\",\\n\",\n",
    "    \"        \\\"Not recommended\\\", \\\"Bad experience\\\", \\\"Poor design\\\", \\\"Unsatisfied\\\"\\n\",\n",
    "    \"    ] * 100\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    neutral_texts = [\\n\",\n",
    "    \"        \\\"Average product\\\", \\\"Okay service\\\", \\\"Nothing special\\\", \\\"Decent quality\\\",\\n\",\n",
    "    \"        \\\"Fair price\\\", \\\"Standard features\\\", \\\"Regular experience\\\", \\\"Normal product\\\"\\n\",\n",
    "    \"    ] * 100\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create DataFrame\\n\",\n",
    "    \"    texts = positive_texts + negative_texts + neutral_texts\\n\",\n",
    "    \"    labels = [2] * len(positive_texts) + [0] * len(negative_texts) + [1] * len(neutral_texts)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Shuffle data\\n\",\n",
    "    \"    indices = np.random.permutation(len(texts))\\n\",\n",
    "    \"    texts = [texts[i] for i in indices]\\n\",\n",
    "    \"    labels = [labels[i] for i in indices]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    df = pd.DataFrame({'text': texts, 'label': labels})\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Split into train, valid, test\\n\",\n",
    "    \"    train_df = df[:1600]\\n\",\n",
    "    \"    valid_df = df[1600:2000]\\n\",\n",
    "    \"    test_df = df[2000:]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    label_mapping = {0: 'negative', 1: 'neutral', 2: 'positive'}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"Synthetic dataset created.\\\")\\n\",\n",
    "    \"    print(f\\\"Label mapping: {label_mapping}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return train_df, valid_df, test_df, label_mapping\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load the dataset\\n\",\n",
    "    \"train_df, val_df, test_df, label_mapping = load_nusax_sentiment_data()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Train set size: {len(train_df)}\\\")\\n\",\n",
    "    \"print(f\\\"Validation set size: {len(val_df)}\\\")\\n\",\n",
    "    \"print(f\\\"Test set size: {len(test_df)}\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nLabel distribution in training set:\\\")\\n\",\n",
    "    \"print(train_df['label'].value_counts().sort_index())\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
